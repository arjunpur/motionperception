{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8e4ea59",
   "metadata": {},
   "source": [
    "# Introduction to Motion Energy Models\n",
    "\n",
    "## Overview\n",
    "\n",
    "Welcome to the first day of our journey into the world of motion energy models! This notebook introduces the fundamental concepts and importance of motion energy models in the context of visual neuroscience.\n",
    "\n",
    "### What we'll cover:\n",
    "- What are motion energy models and why do we study them?\n",
    "- How motion is represented in the brain and in computational models\n",
    "- Historical context and key developments\n",
    "- The biological basis in visual cortex\n",
    "- A roadmap of what we'll build throughout this course"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a21e7d",
   "metadata": {},
   "source": [
    "## Setting Up\n",
    "\n",
    "Let's first import the libraries we'll need throughout the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8c2d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import scipy.signal as signal\n",
    "import sys\n",
    "\n",
    "# Add the utils package to the path\n",
    "sys.path.append('../../..')\n",
    "try:\n",
    "    from motionenergy.utils import stimuli_generation, visualization\n",
    "except ImportError:\n",
    "    print(\"Note: utils modules not found. This is expected if you haven't implemented them yet.\")\n",
    "\n",
    "# For interactive plots\n",
    "%matplotlib inline\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc48a0f",
   "metadata": {},
   "source": [
    "## 1. The Challenge of Motion Perception\n",
    "\n",
    "Think about what happens when you watch a bird flying across the sky, a car driving down the street, or even just waving your hand in front of your face. Your visual system effortlessly detects and tracks these moving objects. But how does your brain do this?\n",
    "\n",
    "### The Problem\n",
    "\n",
    "At its core, your retina is just a two-dimensional array of photoreceptors, each reporting the light intensity at a particular location. Your retina doesn't directly \"see\" motion - it just receives a sequence of static images over time. Yet somehow, your brain transforms this sequence into a robust perception of motion.\n",
    "\n",
    "This is a fundamental computational problem that any visual system must solve:\n",
    "\n",
    "**How do we extract motion information from a sequence of static images?**\n",
    "\n",
    "Let's visualize this problem with a simple animation of a moving dot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0493fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple animation of a moving dot\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.set_aspect('equal')\n",
    "ax.set_title('A Moving Dot')\n",
    "ax.grid(False)\n",
    "\n",
    "dot, = ax.plot([], [], 'ro', markersize=15)\n",
    "\n",
    "def init():\n",
    "    dot.set_data([], [])\n",
    "    return (dot,)\n",
    "\n",
    "def animate(i):\n",
    "    x = i / 10.0\n",
    "    y = 5\n",
    "    dot.set_data(x, y)\n",
    "    return (dot,)\n",
    "\n",
    "anim = animation.FuncAnimation(fig, animate, init_func=init,\n",
    "                               frames=100, interval=50, blit=True)\n",
    "\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a5e66d",
   "metadata": {},
   "source": [
    "## 2. What are Motion Energy Models?\n",
    "\n",
    "Motion energy models offer a computational framework that explains how the brain might solve this problem. They were developed to describe how neurons in the visual cortex could extract motion information from the visual input.\n",
    "\n",
    "### Key Insight\n",
    "\n",
    "The core insight of motion energy models is that motion can be detected by using **spatiotemporal filters** that are selective for both the spatial pattern of the stimulus and how it changes over time.\n",
    "\n",
    "In simpler terms, these models suggest that your brain has specialized neural circuits that act like motion detectors, each tuned to respond preferentially to motion in a particular direction and at a particular speed.\n",
    "\n",
    "### Key Components\n",
    "\n",
    "Motion energy models consist of several key components:\n",
    "\n",
    "1. **Spatiotemporal filtering**: The visual input is processed through filters that are oriented in space-time, making them selective for motion in specific directions\n",
    "\n",
    "2. **Quadrature pairs**: Pairs of filters that are 90° out of phase with each other\n",
    "\n",
    "3. **Energy computation**: Squaring and summing the outputs of these filters to get a phase-invariant measure of motion\n",
    "\n",
    "4. **Opponent processing**: Comparing energy in opposite directions to determine the net motion direction\n",
    "\n",
    "Throughout this course, we'll build an intuitive understanding of each of these components and how they work together to detect motion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e26c6b",
   "metadata": {},
   "source": [
    "## 3. Motion in Space-Time: A New Way of Thinking\n",
    "\n",
    "To understand motion energy models, we need to think about motion in a different way. Instead of thinking about objects changing position over time, we can think about motion as patterns in a three-dimensional space where the dimensions are x, y, and t (time).\n",
    "\n",
    "This 3D representation is called **spatiotemporal space**.\n",
    "\n",
    "Let's visualize this with a simple example. Imagine a vertical bar moving from left to right across a 1D space (a single row of pixels). We can represent this as a 2D space-time plot where the x-axis is space and the y-axis is time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ec2c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a space-time plot of a moving bar\n",
    "width = 100\n",
    "frames = 100\n",
    "bar_width = 5\n",
    "speed = 1\n",
    "\n",
    "# Create an empty space-time array\n",
    "spacetime = np.zeros((frames, width))\n",
    "\n",
    "# Add a moving bar\n",
    "for t in range(frames):\n",
    "    pos = (t * speed) % width\n",
    "    bar_start = int(max(0, pos - bar_width/2))\n",
    "    bar_end = int(min(width, pos + bar_width/2))\n",
    "    spacetime[t, bar_start:bar_end] = 1\n",
    "\n",
    "# Plot the space-time diagram\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Show the first few frames as separate images\n",
    "ax1.set_title('First 5 Frames (What the Retina Sees)')\n",
    "for i in range(5):\n",
    "    # Plot each frame with a different color\n",
    "    ax1.plot(range(width), i * 0.2 + spacetime[i*20, :], label=f'Frame {i+1}')\n",
    "    \n",
    "ax1.set_ylim(0, 2)\n",
    "ax1.set_xlabel('Space (x)')\n",
    "ax1.set_ylabel('Intensity')\n",
    "ax1.legend()\n",
    "\n",
    "# Show the space-time representation\n",
    "ax2.set_title('Space-Time Representation')\n",
    "im = ax2.imshow(spacetime, cmap='gray', aspect='auto', origin='upper')\n",
    "ax2.set_xlabel('Space (x)')\n",
    "ax2.set_ylabel('Time (t)')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9cb28f",
   "metadata": {},
   "source": [
    "## 4. Key Concepts in Motion Energy Models\n",
    "\n",
    "Now that we have a basic understanding of the problem and approach, let's introduce some key concepts that we'll explore in detail throughout the course.\n",
    "\n",
    "### 1. Spatiotemporal Filtering\n",
    "\n",
    "The first step in motion energy computation is to filter the visual input through spatiotemporal filters. These filters are selective for specific patterns in space and time, such as leftward or rightward motion.\n",
    "\n",
    "### 2. Quadrature Pairs\n",
    "\n",
    "Motion energy models use pairs of filters that are 90° out of phase with each other (quadrature pairs). This allows the model to respond to motion regardless of the exact spatial phase of the stimulus.\n",
    "\n",
    "### 3. Energy Computation\n",
    "\n",
    "The outputs of these quadrature pairs are squared and summed to compute a measure of motion energy. This energy is phase-invariant, meaning it responds to motion regardless of the exact appearance of the moving pattern.\n",
    "\n",
    "### 4. Direction Selectivity\n",
    "\n",
    "By comparing the energy in opposite directions (e.g., leftward vs. rightward), the model can determine the net direction of motion in the stimulus.\n",
    "\n",
    "Throughout the course, we'll build each of these components and see how they work together to detect and analyze motion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d5f66d",
   "metadata": {},
   "source": [
    "## 5. Course Roadmap: What's Ahead\n",
    "\n",
    "Here's what we'll be covering in the coming weeks:\n",
    "\n",
    "### Module 1: Foundations (You are here)\n",
    "- Building mathematical intuition for motion representation\n",
    "- Understanding filtering and signal processing fundamentals\n",
    "- Exploring visual stimuli and how they appear in space-time\n",
    "\n",
    "### Module 2: Visual Neuroscience\n",
    "- Exploring how biological vision systems detect motion\n",
    "- Learning about simple and complex cells in the visual cortex\n",
    "- Mapping computational elements to neural structures\n",
    "\n",
    "### Module 3: Core Motion Energy Model\n",
    "- Implementing the complete Adelson & Bergen model\n",
    "- Building spatiotemporal filters and quadrature pairs\n",
    "- Creating direction-selective mechanisms\n",
    "\n",
    "### Module 4: Extensions and Applications\n",
    "- Implementing opponent motion processing\n",
    "- Comparing with other motion detection approaches\n",
    "- Applying models to real-world problems\n",
    "\n",
    "By the end of the course, you'll have a deep understanding of motion energy models, both in theory and in practice, and you'll be able to apply these models to analyze and understand motion perception."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdc7d9c",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this introduction, we've covered:\n",
    "\n",
    "- The fundamental challenge of motion perception: extracting motion from a sequence of static images\n",
    "- The key insight of motion energy models: using spatiotemporal filters to detect motion\n",
    "- The spatiotemporal representation of motion: visualizing motion as oriented patterns in space-time\n",
    "- An overview of the key components of motion energy models\n",
    "- A roadmap of what we'll be learning throughout the course\n",
    "\n",
    "In the next section, we'll dive deeper into visual stimuli used to study motion perception, which will allow us to create and manipulate motion patterns that we can analyze with our models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a43a28",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "If you're interested in learning more about motion energy models, here are some resources to check out:\n",
    "\n",
    "### Key Papers\n",
    "- Adelson, E. H., & Bergen, J. R. (1985). Spatiotemporal energy models for the perception of motion. *Journal of the Optical Society of America A*, 2(2), 284-299.\n",
    "- Watson, A. B., & Ahumada, A. J. (1985). Model of human visual-motion sensing. *Journal of the Optical Society of America A*, 2(2), 322-342.\n",
    "\n",
    "### Books\n",
    "- Wandell, B. A. (1995). *Foundations of Vision*. Sinauer Associates.\n",
    "- Dayan, P., & Abbott, L. F. (2001). *Theoretical Neuroscience: Computational and Mathematical Modeling of Neural Systems*. MIT Press.\n",
    "\n",
    "See the full list of resources in the [references section](../../../references/additional_resources.md)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "motionperception",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
